\documentclass[11pt,a4paper,notitlepage]{scrreprt}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{ulsy}
\usepackage{tikz}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath,amssymb,amstext,amsfonts,mathrsfs, amsthm}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{color}
\usepackage{enumitem}
\usepackage{framed}


% Mathmatische Symbole
\newcommand{\RR}{\mathbb{R}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\ee}{\operatorname{e}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\grad}{\operatorname{grad}}

\newtheorem{defi}{Definition}[section]
\newtheorem{prop}[defi]{Proposition}
\newtheorem{theorem}[defi]{Theorem}
\newtheorem{cor}[defi]{Corollar}
\newtheorem{lem}[defi]{Lemma}
\newtheorem{bem}[defi]{Bemerkung}

\author{ich}
\title{Seminar 0.0}



\begin{document}
\parindent 0pt


%\maketitle
%\newpage
%\tableofcontents
%\newpage

\pagestyle{plain}



 Prof. Dr. Matthes \hfill Manuela Lambacher\\
 Optimaler Transport - Geometrie und Anwendungen \hfill 09.12.2015
 \begin{center}
  {\huge{Zeit-Diskretisierung von Gradientenflüssen}} 
 \end{center}
 
 \hrule
 
\numberwithin{equation}{section}
\renewcommand{\thechapter}{\arabic{section}}
\renewcommand{\thesection}{\arabic{section}}
\section{Generalisierte Gradientenflüsse}

Wir betrachten folgenden Gradientenfluss:
\begin{eqnarray}
\dfrac{dX}{dt}=~-\grad E(X) \label{eq1}
\end{eqnarray}
$E$ ist ein Energie-Funktional, $X(t)$ können wir z.B. als Position gegebener Partikel interpretieren.\newline


Statt dieses Problem explizit zu lösen, ist unser Ziel, den Gradientenfluss als Grenzwert eines zeitlich diskretisierten Problems darzustellen, sodass jeglicher expliziter Umgang mit Subdifferentialen und dem Gradientenoperator umgangen wird.

\begin{defi}\label{approx}
Der \textbf{approximierte Gradientenfluss} $(X_\tau)$ für ein geeignetes Energie-Funktional E in einem abstraktem metrischen Raum mit einer Distanz sei gegeben durch:
\newline
Sei die zeitliche Schrittweite $\tau > 0$ gegeben, $n\in\NN_0$, dann ist die Folge $\left( X^n_\tau \right)_{n\geq 0}$:
\\
$X_\tau^0:=X_0,$
\\
$X_\tau^{n+1}$ ist ein Minimierer des Minimerungsproblems
\begin{eqnarray}
\min\left[E(X)+\dfrac{\dist(X_\tau^n,X)^2}{2\tau}\right] \label{Min}
\end{eqnarray}
Sei $X_\tau$ auf $\RR_+$ als stückweise konstante Funktion mit Wert $X_\tau(t)=X^n_\tau$ für $t\in [n\tau,(n+1)\tau)$ definiert.
\end{defi}

\begin{bem}
\begin{enumerate}
\item Wir setzen zunächst keine Eindeutigkeit des Minimums voraus;
\item Existenz des Minimierers muss durch geeignete Eigenschaften von $E(X)$ gesichert werden, z.B. Koerzivität, Weakly lower semicontinuity und eine untere Schranke bzw. Wachstumsbedingung. 
\item Verdeutlichen wir uns, warum dies unser anfängliches Problem approximieren soll:\\
Betrachtet man den euklidischen Abstand, so besitzt das Problem (\ref{Min}) einen kritischen Punkt (der für $E(X)$ konvex ein globales Minimum ist) bei $X_\tau^{n+1}$ gegeben durch 
\begin{eqnarray*}
\dfrac{X_\tau^n-X_\tau^{n+1}}{\tau}=-\grad E(X_\tau^{n+1}),
\end{eqnarray*}
wobei die linke Seite offensichtlich eine Approximation der zeitlichen Ableitung von $X(t)$ ist.
\end{enumerate}
\end{bem}
\vspace{3pt}
Nach Aufstellen der Diskretisierung wollen wir $\tau \to 0$ gehen lassen und betrachten den Grenzwert, den "'generalisierten Gradientenfluss"'.
Um zu zeigen, dass dieser Grenzwert exisitert und die Ausgangsgleichung erfüllt, wollen wir zunächst einige Eigenschaften untersuchen. 

\subsection{Drei Ungleichungen}
Sei $\Vert \cdot \Vert$ die (euklidische) Norm. Für den nächsten Abschnitt verwenden wir der Anschaulichkeit halber den dadurch induzierten euklidischen Abstand,, verwenden jedoch keine Eigenschaften, die über diejenigen jeder anderen beliebigen norminduzierten Distanz hinausgehen, sodass die Aussagen für jede Distanz gelten.\\
 Wir nehmen an, dass $E$ nach unten durch eine absolute Konstante beschränkt ist, d.h. $E(X)\geq C$ unabh. von $X$.
\newline


$X_\tau^{n+1}$ ist ein Minimierer des Funktionals $X \mapsto E(X)+\dfrac{\Vert X_\tau^n-X\Vert^2}{2\tau}$. Damit ist offensichtlich das Funktional an der Stelle $X_\tau^{n+1}$ kleiner gleich dessen Wert an der Stelle $X_\tau^n$ ($\Vert X_\tau^n-X_\tau^n\Vert=0$). Damit haben wir
\begin{eqnarray}
E(X_\tau^{n+1})+\dfrac{\Vert X_\tau^n-X_\tau^{n+1}\Vert^2}{2\tau}\leq E(X_\tau^n). \label{ineq1}
\end{eqnarray} 
\begin{lem}[Energy estimate]
\begin{eqnarray}
\sup_{n\geq 0}E(X_\tau^n)\leq E(X^0) \label{enest}
\end{eqnarray}
\end{lem}
\begin{proof}
Folgt sofort aus (\ref{ineq1}), da der zweite Term nichtnegativ und somit die Folge der $(E(X_\tau^n))$ für steigendes n monoton abnehmend ist. 
\end{proof}

\begin{lem}[total square distance estimate]
\begin{eqnarray}
\sum_{n\geq 0}\Vert X_\tau^n-X_\tau^{n+1}\Vert^2\leq 2\tau (E(X^0)-\inf E)\label{totalsquare}
\end{eqnarray}
\end{lem}
\begin{proof}
Summation über (\ref{ineq1}), Teleskopsumme, Infimum da $E(X_\tau^n)$ monoton fallend.
\end{proof}

\begin{lem}[Hölder 1/2-Abschätzung für $X_\tau$]  $~~$ \\
Für $s<t$ gilt:
\begin{eqnarray}
\Vert X_\tau(s)-X_\tau(t)\Vert^2 \overset{1)}\leq \left[\dfrac{t-s}{\tau}+1\right] \sum_{\frac{s}{\tau}\leq n \leq \frac{t}{\tau}} \Vert X^n_\tau- X_\tau^{n+1}\Vert^2 \overset{2)}\leq 2(E(X^0)-\inf E)[(t-s)+\tau]. \label{Hölder}
\end{eqnarray}
\end{lem}

\begin{proof}
\begin{itemize}
\item[2)] folgt wiederum aus Aufsummation von (\ref{ineq1}) und Monotonie.
\item[1)] Nach Definition: $X_\tau(s)=X_\tau^n$ für $s\in [n\tau,(n+1)\tau)$. Damit gilt für $n$: $n=\lfloor \frac{s}{	\tau}\rfloor$. Analog sei $X_\tau(t)=X_\tau^m$, $m=\lfloor\frac{t}{\tau}\rfloor+1$.\\
\begin{align*}
\Vert X_\tau(s)-X_\tau(t)\Vert=&\Vert X_\tau^n-X_\tau^m\Vert \\\overset{\triangle -Ungl.}\leq&\Vert X_\tau^n-X_\tau^{n+1}\Vert+\Vert X_\tau^{n+1}-X_\tau^{n+2}\Vert+...+\Vert X_\tau^{m-1}-X_\tau^m\Vert\\
=&\sum_{n\leq k\leq m-1}\Vert X_\tau^k-X_\tau^{k+1}\Vert\\
\Vert X_\tau(s)-X_\tau(t)\Vert^2\leq&\Bigg(\sum_{\frac{s}{\tau}\leq k\leq \frac{t}{\tau}}\Vert X_\tau^k-X_\tau^{k+1}\Vert\Bigg)^2 \\ \overset{CS}\leq&\Bigg(\sum_{\frac{s}{\tau}\leq k\leq \frac{t}{\tau}}\Vert X_\tau^k-X_\tau^{k+1}\Vert^2\Bigg)\underset{\frac{t}{\tau}-(\frac{s}{\tau}-1)}{\underbrace{\Bigg(\sum_{\frac{s}{\tau}\leq k\leq \frac{t}{\tau}}1\Bigg)}}
\end{align*}
\end{itemize}
\end{proof}
Diese drei Abschätzungen stellen uns die relative Kompaktheit von $(X_\tau)$ für $\tau \to 0$ sicher, und damit die Existenz einer konvergenten Teilfolge.  \\





\subsection{Perturbationen oder löst mein Grenzwert den Gradientenfluss?}

Sei eine zugrundeliegende Riemannstruktur (Differenzierbare Mannigfaltigkeit, Metrik, Tangentenräume) gegeben und das Energie-Funktional hinreichend glatt. \\
Wir wollen nun noch einmal betrachten, warum die Lösung unseres Minimerungsproblem einen Gradientenfluss approximiert. Dabei wird eine Strategie eingeführt, der auch - in einem etwas umfangreicherem Setting - der nächste Abschnitt über die Fokker-Planck-Gleichung folgt. \\
Wir befinden uns im $\RR^k$. Sei $\omega$ ein beliebiger Tangentenvektor auf $X_\tau^{n+1}$. Wir betrachten Perturbationen von $X_\tau^{n+1}$ "'entlang $\omega$"': Definiere für ein kleines $\varepsilon>0$ einen "'Pfad"' $\tilde{X}_\varepsilon$ folgendermaßen: \\
\begin{eqnarray}
\tilde{X}_0=X_\tau^{n+1},~~\dfrac{d\tilde{X}_\varepsilon}{d\varepsilon}\Bigg|_{\varepsilon=0} =\omega
\end{eqnarray}
Essentiell für die folgenden Schritte ist, dass die Ableitung an der Stelle $\varepsilon$ gleich Null ist. Der Einfachheit halber nehmen wir die lineare Form $\tilde{X}_\varepsilon=X_\tau^{n+1}+\varepsilon\omega$ an.\\
Anmerkung: Wenn wir allgemeinere Perturbationen betrachten wollen, verfahren wir ganz analog, indem wir nach Taylor \begin{eqnarray}
\tilde{X}_\varepsilon = X_0+\dfrac{d\tilde{X}_\varepsilon}{d\varepsilon}\Bigg|_{\varepsilon=0}\cdot\varepsilon+O(\varepsilon^2),\\
E(\tilde{X}_\varepsilon)=E(X_\tau^{n+1})+\varepsilon\langle \grad E(X_\tau^{n+1}),\omega\rangle+O(\varepsilon^2) \label{b)}
\end{eqnarray}
verwenden.\\
Da $X_\tau^{n+1}$ Minimierer ist, gilt offensichtlich
\begin{eqnarray}
E(X_\tau^{n+1})+\dfrac{\Vert X_\tau^n-X_\tau^{n+1}\Vert^2}{2\tau}\leq E(\tilde{X}_\varepsilon)+\dfrac{\Vert X_\tau^n-\tilde{X}_\varepsilon\Vert^2}{2\tau}~~~\forall \varepsilon>0 \label{a)}\\
\overset{\cdot\frac{1}{\varepsilon}}\Leftrightarrow \dfrac{E(X_\tau^{n+1})-E(\tilde{X}_\varepsilon)}{\varepsilon}\leq \dfrac{\Vert X_\tau^n-\tilde{X}_\varepsilon\Vert^2-\Vert X_\tau^n-X_\tau^{n+1}\Vert^2}{2\tau\varepsilon}\\
\Leftrightarrow -\dfrac{E(X_\tau^{n+1}+\varepsilon\omega)-E(X_\tau^{n+1})}{\varepsilon}\leq \dfrac{\Vert X_\tau^n-X_\tau^{n+1}+\varepsilon\omega\Vert^2-\Vert X_\tau^n-X_\tau^{n+1}\Vert^2}{2\tau\varepsilon}
\end{eqnarray}
Wir schreiben nun vor beide Seiten $\lim_{\varepsilon\to 0}$ und interpretieren diese Differenzenquotienten als Ableitungen nach $\varepsilon$. Wir erhalten:
\begin{equation}
-\grad E(X_\tau^{n+1})\cdot\omega\leq \dfrac{\langle X_\tau^n-X_\tau^{n+1},\omega\rangle}{\tau}
\end{equation}
Gleichheit wird durch die Betrachtung von $-\omega$ erreicht, und damit haben wir eine schwache Formulierung unsers approximierten Gradientenflusses.\\
(Für explizite $\omega$ können wir auch viele weitere Ungleichungen herleiten.)

 
\newpage

\begin{lem}[Energy gradient estimate]
Sei eine zugrundeliegende Riemannstruktur (Differenzierbare Mannigfaltigkeit, Metrik, Tangentenräume) gegeben und das Energie-Funktional hinreichend glatt. Dann gilt 
\begin{eqnarray}
\tau \sum_{n\geq 0} \Vert\grad E(X_\tau^n)\Vert^2 \leq 2[E(X^0)-\inf E]
\end{eqnarray}
bzw. im stetigen Falle für $E_\tau$ (nach dessen Definition):
\begin{eqnarray}
\int_0^\infty \Vert\grad E(X_\tau)\Vert^2 dt\leq 2[E(X^0)-\inf E]
\label{Engrad}
\end{eqnarray}
\end{lem}
\begin{proof}
Sei $\omega$ ein beliebiger Tangentenvektor auf $X_\tau^{n+1}$. Wir betrachten Perturbationen von $X_\tau^{n+1}$ "'entlang $\omega$"': Definiere für ein kleines $\varepsilon>0$ einen "'Pfad"' $\tilde{X}_\varepsilon$ folgendermaßen:\\
\begin{eqnarray}
\tilde{X}_0=X_\tau^{n+1},~~\dfrac{d\tilde{X}_\varepsilon}{d\varepsilon}\Bigg|_{\varepsilon=0} =\omega
\end{eqnarray}
Da $X_\tau^{n+1}$ Minimierer ist, gilt offensichtlich
\begin{eqnarray}
E(X_\tau^{n+1})+\dfrac{\dist(X_\tau^n,X_\tau^{n+1})^2}{2\tau}\leq E(\tilde{X}_\varepsilon)+\dfrac{\dist(X_\tau^n,\tilde{X}_\varepsilon)^2}{2\tau}~~~\forall \varepsilon>0 \label{a)}
\end{eqnarray}
Unter der Annahme, dass das Energiefunktional glatt ist, gilt, jeweils mit Taylor:
\begin{eqnarray}
\tilde{X}_\varepsilon = X_0+\dfrac{d\tilde{X}_\varepsilon}{d\varepsilon}\Bigg|_{\varepsilon=0}\cdot\varepsilon+O(\varepsilon^2),\\
\begin{split}
E(\tilde{X}_\varepsilon)=&E(X_0)+\langle\grad E(X_0),\underset{\omega\varepsilon+O(\varepsilon^2)}{\underbrace{\tilde{X}_\varepsilon -X_0}}\rangle+O((\tilde{X}_\varepsilon-X_0)^2)\\
=&E(X_\tau^{n+1})+\varepsilon\langle \grad E(X_\tau^{n+1}),\omega\rangle+O(\varepsilon^2) \label{b)}
\end{split}
\end{eqnarray}

Außerdem gilt:
\begin{align*}
\dist (X_\tau^n,&\tilde{X}_\varepsilon)^2-\dist(X_\tau^n,X_\tau^{n+1})^2=\\
\overset{binom.}=&\left(\dist(X_\tau^n,\tilde{X}_\varepsilon)+\dist(X_\tau^n,X_\tau^{n+1})\right)\left(\dist(X_\tau^n,\tilde{X}_\varepsilon)-\dist(X_\tau^n,X_\tau^{n+1})\right)\\
\overset{\triangle-Ungl.}\leq& \left(\dist(X_\tau^n,\tilde{X}_\varepsilon)+\dist(X_\tau^n,X_\tau^{n+1})\right)\dist(X_\tau^{n+1},\tilde{X}_\varepsilon) \\
\overset{\triangle-Ungl.}\leq&\left(2\dist(X_\tau^n,X_\tau^{n+1}+\dist(X_\tau^{n+1},\tilde{X}_\varepsilon)\right)\dist(X_\tau^{n+1},\tilde{X}_\varepsilon)
\end{align*}
Beobachtung: \[\dist(X_\tau^{n+1},\tilde{X}_\varepsilon)=\varepsilon\Vert \omega \Vert + o(\varepsilon). \]
Damit wird aus obiger Gleichung
\begin{eqnarray}
\dfrac{\dist(X_\tau^n,\tilde{X}_\varepsilon)^2}{2\tau}\leq \dfrac{\dist(X_\tau^n,X_\tau^{n+1})^2}{2\tau}+\varepsilon\dist(X_\tau^n,X_\tau^{n+1})\dfrac{\Vert \omega \Vert}{\tau} + o(\varepsilon) \label{c)}
\end{eqnarray}
\\
Jetzt kombinieren wir die drei Aussagen (\ref{a)}),(\ref{b)}) und (\ref{c)}): Dazu setzen wir zunächst (\ref{b)}) in (\ref{a)}) ein.
\begin{align*}
E&(X_\tau^{n+1})+\dfrac{\dist(X_\tau^n,X_\tau^{n+1})^2}{2\tau}\leq E(X_\tau^{n+1})+\varepsilon\langle \grad E(X_\tau^{n+1}),\omega\rangle+O(\varepsilon^2)+\dfrac{\dist(X_\tau^n,\tilde{X}_\varepsilon)^2}{2\tau}
\\
\Rightarrow& \dfrac{\dist(X_\tau^n,X_\tau^{n+1})^2}{2\tau}\overset{\ref{c)}}{\leq} \varepsilon\langle \grad E(X_\tau^{n+1}),\omega\rangle+O(\varepsilon^2)+ \dfrac{\dist(X_\tau^n,X_\tau^{n+1})^2}{2\tau}+\varepsilon\dist(X_\tau^n,X_\tau^{n+1})\dfrac{\Vert \omega \Vert}{\tau} + o(\varepsilon)\\
\Rightarrow& 0\leq \varepsilon \bigg(\langle \grad E(X_\tau^{n+1}),\omega\rangle)+\dist(X_\tau^n,X_\tau^{n+1})\dfrac{\Vert \omega \Vert}{\tau} +\underset{\to 0 \text{ für } \varepsilon \to 0} {\underbrace{\dfrac{o(\varepsilon)}{\varepsilon}+\dfrac{O(\varepsilon^2)}{\varepsilon}}} \bigg)
\end{align*}
Nun wählen wir $\omega$ explizit: $\omega=-\grad E(X_\tau^{n+1})$ (im Tangentialraum als Ableitung eines Funktionals auf dem Vektorraum)
\begin{align*}
0\leq& \underset{-\Vert \grad E(X_\tau^{n+1}) \Vert^2}{\underbrace{\langle \grad E(X_\tau^{n+1}),-\grad E(X_\tau^{n+1}\rangle)}}+\dfrac{\dist(X_\tau^n,X_\tau^{n+1})\Vert \grad E(X_\tau^{n+1}) \Vert}{\tau}\\
\Rightarrow& \dfrac{\dist(X_\tau^n,X_\tau^{n+1})^2}{\tau^2} \geq \Vert \grad E(X_\tau^{n+1}) \Vert^2
\end{align*}
In Verbindung mit der square distance estimate (\ref{totalsquare}) kommen wir schließlich zu der geforderten Gleichung:
\begin{equation}
 2\tau (E(X^0)-\inf E)\overset{\ref{totalsquare}}\geq \sum_{n\geq 0} \dist(X_\tau^n,X_\tau^{n+1})^2 \geq \tau^2 \sum_{n\geq 1} \Vert \grad E(X_\tau^n)\Vert^2
\end{equation}
TODO: warum steht im Buch Summe über $n\geq 0$? Ist auf $\dist$-Seite nicht definiert.
\end{proof}
\begin{bem}
Die Aussage des Lemmas für $X_\tau$ gilt auch in einer nicht-riemannschen Situation mit einer passenden Definition der Norm des Gradienten. 
\end{bem}

Diese drei Abschätzungen stellen uns die relative Kompaktheit von $(X_\tau)$ für $\tau \to 0$ sicher, und damit die Existenz einer konvergenten Teilfolge.  \\
Um abschließend zum Limes überzugehen, müssen wir ein dem letzten Beweis ähnliches Verfahren durchlaufen: Kleine Perturbationen, $\omega$ beliebig lassen. Approximierte Gleichung finden. Siehe nächster Abschnitt. 

\newpage
\section{Die lineare Fokker-Plank-Gleichung, Monge-Kantorovich und der approximierte Gradientenfluss}

(Notation in diesem Abschnitt: $\rho=\rho(x,t)$)\\\\
Diese Strategie wurde zum ersten Mal am Beispiel der linearen Fokker-Plank-Gleichung angewandt. \\
\begin{defi}
Die \textbf{lineare Fokker-Planck-Gleichung} (LFP) mit glattem Potential $V(x):\RR^k\to[0,\infty)$ und Anfangswert $\rho_0$, Wahrscheinlichkeitsdichte auf $\RR^k$, ist gegeben durch:
\begin{eqnarray}
\begin{split}
\dfrac{\partial\rho}{\partial t}=\Delta\rho+\nabla\cdot(\rho\nabla V),\\
\rho(0)=\rho_0.\label{FP}
\end{split}
\end{eqnarray}
Alle Lösungen $\rho(x,t)$ sind ebenfalls Wahrscheinlichkeitsdichten auf $\RR^k$ für fast alle festen $t$, d.h. $\rho(x,t)\geq 0$ für fast alle $(x,t)\in \RR^k\times(0,\infty)$  und $\int_{\RR^k}\rho(x,t)dx=1$ für fast alle $t\in(0,\infty)$
Eine Wahrscheinlichkeitsdichte $\rho$ erfüllt die (LFP) im schwachen Sinne, falls für alle Testfunktionen $\zeta$ gilt:
\begin{equation}
\int_{\RR^k} \int_T (\rho\Delta\zeta+\rho\nabla V\cdot\nabla\zeta -\rho\zeta_t) dt dx=\int\rho_0\zeta(x,0)dx \label{FPweak}
\end{equation}
\end{defi}

Die Fokker-Planck Gleichung beschreibt die zeitabhängige Entwicklung einer Wahrscheinlichkeitsdichte für einen stochastischen Prozess. Sie wird verwendet, um die Wahrscheinlichkeitsdichte für Ort oder Geschwindigkeit eines Partikels zu beschreiben, das verschiedenen Kräften ausgesetzt ist.

\begin{bem}
Wenn $V$ geeignete Wachstumsbedingungen erfüllt, existiert eine eindeutige stationäre Lösung zur (LFP).
\end{bem}
Auch wenn die explizite Lösung der Gleichung bereits bekannt war, liefert die Anwendung des hier zu zeigenden Approximationsverfahren interessante Einblicke in die Methode, die auch für andere Diffgleichungen und in numerischer Implementierung Anwendung findet. 


Wir konstruieren ein zeitlich diskretisiertes, iteratives variationelles Verfahren (d.h. die Lösung minimiert ein bestimmtes konvexes Funktional), dessen Lösungen gegen die Lösung der linearen Fokker-Planck-Gleichung konvergieren.

\begin{defi}
Für eine Wahrscheinlichkeitsdichte $\rho\in P(\RR^k)$ und ein hinreichend glattes Potential V sei das \textbf{Freie Energie Funktional $E(\rho)$} gegeben durch:
\begin{equation}
E(\rho):=\int_{\RR^k} \rho\log\rho dx + \int_{\RR^k}\rho V dx \label{FEFktn}
\end{equation}
\end{defi}

\begin{bem}
\begin{itemize}
\item Das Freie Energie Funktional ist wohldefiniert für Lösungen $\rho(x,t)$ der (LFP), falls $E(\rho_0)<\infty$.
\item $E(\rho(x,t))$ ist für Lösungen des (LFP) bzgl. der Zeit monoton abnehmend.\end{itemize}
\item Nach der Definition des letzten Vortrages betrachten wir die Summe aus interner und potentieller Energie.
\end{bem}

Wir wollen die Beziehung zwischen Fokker-Planck und dem freien Energie-Funktional zeigen, d.h. dass die Dynamik als Gradientenfluss interpretiert werden kann für die Frei Energie mit der Wassersteinmetrik, d.h. die Lösung der Gleichung folgt der Richtung des steilsten Abstieges des Funktionals. Die für dieses Setting benötigte Metrik wird die bereits eingeführte Wassersteinmetrik sein. \\

Wiederholung:
\begin{defi}
Seien $f,g\in\mathcal{P}(\RR^k)$ zwei Wahrscheinlichkeitsmaße (Notation: bzw. ihre assoziierten Wahrscheinlichkeitsdichten.)
\begin{equation}
W_2(f,g):=\inf_{\pi \in \Pi(f,g)} \left\{ \int_{\RR^k\times\RR^k} \vert x-y \vert^2 d\pi(x,y)\right\}^\frac{1}{2}.\label{W2}
\end{equation}
$\Pi(f,g)$: Menge aller multivariaten Wahrscheinlichkeitsmaße auf $\RR^k \times \RR^k$ mit Randverteilungen f und g.
\end{defi}
Da die Wassersteinmetrik zusammen mit dem Raum aller Wahrscheinlichkeitsmaße aber keine Riemann'sche Mannigfaltigkeit ist, und damit der Gradient zunächst nicht wohldefiniert ist, müssen wir analog zum ersten Teil des Vortrags zeitlich diskretisieren.\\


\begin{theorem}
Sei $V(x)$ ein glattes Potential, das folgende Wachstumsbedingung erfüllt: \\$V(x)=O(\vert x \vert^2)$ für $x\to\infty$. \\
Sei $\rho_0$ eine Wahrscheinlichkeitsdichte, sodass das Freie Energie Funktional $E(\rho)$ (\ref{FEFktn}) an der Stelle $\rho_0$ endlich ist. Sei $\tau>0$.\\
Dann gilt: \\
Der approximierte Gradientenfluss $\rho_\tau$  für $E(\rho)$ bzgl. der quadratischen Wassersteindistanz konvergiert für $\tau\to 0$ für alle $t\in(0,\infty)$ schwach in $L^1(\RR^k)$ gegen die eindeutige Lösung $\rho(x,t)\in C^\infty((\RR^k\times(0,\infty))$ der linearen Fokker-Planck-Gleichung (\ref{FP}) mit Anfangswert $\rho_0$.\\\\
Der approximierte Gradientenfluss (bzw. die Interpolation) $\rho_\tau:(0,\infty)\times\RR^k\to[0,\infty)$ wird hierbei analog zu Def \ref{approx} konstruiert:
\begin{equation}
\rho_\tau(t):=\rho_\tau^n ~\text{ für }t\in[n\tau,(n+1)\tau)
\end{equation}
für die Folge $(\rho_\tau^n)_n,~n\in\NN_0$:
\[\rho_\tau^0=\rho^0; \]
Gegeben $\rho_\tau^n$, definiere den nächsten Schritt als Minimierer von
\begin{equation} \rho \mapsto E(\rho)+\dfrac{W_2(\rho_\tau^n,\rho)^2}{2\tau}, \label{rho^n}
\end{equation}
Das heißt: Die Fokker-Planck-Gleichung ist der Gradientenfluss für das Freie-Energie Funktional mit der Wassersteindistanz.
\end{theorem}

\begin{proof}
\begin{enumerate}
\item Da E strikt konvex im üblichen Sinne ist, ebenso wie die Wassersteinmetrik, ist der Minimierer in (\ref{rho^n})  eindeutig. Auch möglich: nach dem letzten Vortrag ist für $V$ konvex das Funktional displacement konvex. 
\item Analog zu (\ref{enest}) erhält man eine obere Grenze für unser Funktional: $\sup_{t\geq 0} E(\rho_\tau(t))=\sup_n E(\rho_\tau^n)\leq E(\rho_0)$
Dies garantiert die Straffheit der Familie $(\rho_\tau)_{\tau >0}$ in der x-Variable sowie schwache $L^1$-Kompaktheit.\\
Die Hölder 1/2 Ungleichung (\ref{Hölder}) hinsichtlich der Wassersteindistanz sichert gleichgradige Stetigkeit bzgl. $t$ von $(\rho_\tau)_\tau$. ($W_2(\rho_\tau(s)-\rho_\tau(t))<\varepsilon \forall \vert s-t\vert<\delta$; $\forall \varepsilon$ setze $\delta=\frac{\varepsilon^2}{2(E(\rho_0)-\inf E)}-\tau$)\\\\
$\Rightarrow$ All dies ist hinreichend, damit(betrachte falls nötig Teilfolge)$(\rho_\tau)_\tau$ zu einer Funktion $\rho: \RR_+ \mapsto P_{ac}(\RR^k)$ konvergiert, stetig falls $P_{ac}(\RR^k)$ mit der schwachen $L^1$-Topologie verknüpft ist. \\
\item Erfüllt nun dieser Grenzwert den Satz, d.h. die (LFK)? \\
Wir betrachten eine geeignete Variation von $\rho_\tau^{n+1}$. Ein wenig komplexer als in Abschnitt 1, um unserer Wassersteindistanz gerecht zu werden, jedoch mit dem gleichen Ziel.\\
Sei $\xi$ ein glattes Vektorfeld mit kompaktem Träger, und $T_\varepsilon$ eine Familie von Kurven verknüpft mit dem Vektorfeld $\operatorname{Id} +\varepsilon\xi$. Dann definiere folgende Perturbation:
\[\tilde{\rho}_\varepsilon=T_\varepsilon \#\rho_\tau^{n+1} \]
(Pushforward: $(\tilde{\rho}_\varepsilon \mathcal{L}^k)(B)=(\rho_\tau^{n+1}\mathcal{L}^k)(T_\varepsilon^{-1}(B))$)\\
Für $\varepsilon$ klein genug ist $T_\varepsilon$ ein $C^1$-Diffeomorphismus) und invertierbar ($\det(\nabla T_\varepsilon)>0$.\\
\item Es gilt aufgrund der Invertierbarkeit von $T_\varepsilon$ mithilfe von Substitution und McCann-Theorem (Vortrag Alex):  
\begin{align}
E(\tilde{\rho}_\varepsilon)=&\int\tilde{\rho}_\varepsilon\log\tilde{\rho}_\varepsilon+\int\tilde{\rho}_\epsilon V\\
=&\int \rho_\tau^{n+1}\log\dfrac{\rho_\tau^{n+1}}{\det(I_k\varepsilon \nabla \xi)}+\int \rho_\tau^{n+1}(x)V(x+\varepsilon\xi(x))dx.
\end{align}
\item Aus der Energy Estimate folgt andererseits, dass $\rho_\tau^n, \rho_\tau^{n+1}$ absolut stetig (bzgl. Lebesgue-maß)) sind. Damit (Brenier, ebenfalls Vortrag Alex) existert eine bzgl. der Wassersteinmetrik optimale Abbildung $\nabla \phi$ sodass $\nabla \phi \# \rho_\tau^n=\rho_\tau^{n+1}$, also gilt
\begin{equation}
W_2(\rho_\tau^n,\rho_\tau^{n+1})^2=\int \rho_\tau^n(x)\vert x-\nabla\phi(x)\vert^2 dx.
\end{equation}
Dann können wir $\tilde{\rho_\varepsilon}$ auch in Abhängigkeit von $\rho_\tau^n$ statt $\rho_\tau^{n+1}$ schreiben: $\tilde{\rho}_\varepsilon=[(\operatorname{Id}+\varepsilon\xi)\circ\nabla\phi]\#\rho_\tau^n$.\\
Wir verwenden nun nach der Definition der Wassersteinmetrik analog zum letzten Schritt die bestimmte Abbildung $[(\operatorname{Id}+\varepsilon\xi)\circ\nabla\phi]$ zwischen $\tilde{\rho}_\varepsilon$ und $\rho_\tau^n$, diesmal haben wir allerdings keinen Anspruch darauf, dass dies tatsächlich die optimale wäre, und erlangen so folgende Ungleichung:
\begin{equation*}
W_2(\rho_\tau^n,\tilde{\rho}_\varepsilon)^2 \leq \int \vert x-\underset{[(\operatorname{Id}+\varepsilon\xi)\circ\nabla\phi](x)}{\underbrace{\nabla \phi(x)-\varepsilon\xi\circ\nabla\phi(x)}}\vert^2 \rho_\tau^n(x)dx
\end{equation*}
Damit kommen wir auf
\begin{align}
\begin{split}
0\leq\dfrac{W_2(\rho_\tau^n,\tilde{\rho}_\varepsilon)^2}{2\tau}&+E(\tilde{\rho}_\varepsilon)-\dfrac{W_2(\rho_\tau^n,\rho_\tau^{n+1})^2}{2\tau}-E(\rho_\tau^{n+1})\\
\overset{\log(\frac{a}{b})=\log a-\log b}\leq&\int\rho_\tau^n(x)\left(\dfrac{\vert x-\nabla \phi(x)-\varepsilon\xi\circ\nabla\phi(x)\vert^2-\vert x-\nabla\phi(x)\vert^2}{2\tau}\right)dx\\
+\int\rho_\tau^{n+1}&(x)[V(x+\varepsilon\xi(x))-V(x)]dx-\int\rho_\tau^{n+1}(x)\log( \det(I_k+\varepsilon\nabla\xi(x)))dx \label{inequ}
\end{split}
\end{align}
wobei die Nichtnegativität aus der Minimierungseigenschaft von $\rho_\tau^{n+1}$ folgt.\\
Nun dividieren wir durch $\varepsilon$ und lassen dieses gegen $0^+$ gehen, analog zu Abschnitt 1. Die sich ergebenden "'Differenzenquotienten"' formulieren wir als Ableitungen an der Stelle $\varepsilon=0$. Damit wird (\ref{inequ}) zu: 
\begin{align*}
0\leq& \lim_{\varepsilon\to 0^+} \Bigg(\dfrac{1}{2\tau} \int\left(\dfrac{\vert x-\nabla \phi(x)-\varepsilon\xi\circ\nabla\phi(x)\vert^2-\vert x-\nabla\phi(x)\vert^2}{\varepsilon}\right)\rho_\tau^n(x)dx\\
+&\int\frac{1}{\varepsilon}[V(x+\varepsilon\xi(x))-V(x)]\rho_\tau^{n+1}(x)dx-\int\dfrac{\log( \det(I_k+\varepsilon\nabla\xi(x)))-\log(\det I_k)}{\varepsilon}\rho_\tau^{n+1}(x)dx \Bigg)\\
=& \dfrac{1}{2\tau} \int \dfrac{\partial}{\partial\varepsilon}\vert x-\nabla\phi-\varepsilon\xi\circ\nabla\phi(x)\vert^2\Big|_{\varepsilon=0} \rho_\tau^n(x)dx
+\int\frac{\partial}{\partial\varepsilon}[V(x+\varepsilon\xi(x))]\Big|_{\varepsilon=0}\rho_\tau^{n+1}(x)dx\\-&\int\dfrac{\partial}{\partial\varepsilon}[\log(\underset{=1+\operatorname{tr}\varepsilon\nabla\xi(x)+O(\varepsilon^2(\nabla\xi(x)))^2}{\underbrace{\det(I_k+\varepsilon\nabla\xi(x))}})]\Big|_{\varepsilon=0} \rho_\tau^{n+1}(x)dx \\
=&\frac{1}{\tau} \int \rho_\tau^n(x)\left\langle\nabla\phi(x)-x,\xi\circ\nabla\phi(x)\right\rangle dx+\int\rho_\tau^{n+1}(x)\langle\nabla V(x),\xi(x)\rangle dx-\int\rho_\tau^{n+1}(x)1\cdot\underset{\nabla\cdot\xi}{\underbrace{(\operatorname{tr}\nabla\xi)}}(x)dx
\end{align*}
Da dies genauso für $-\xi$ gilt, bekommen wir Gleichheit. Umformuliert:
\begin{equation}
\frac{1}{\tau} \int \rho_\tau^n(x)\left\langle\nabla\phi(x)-x,\xi\circ\nabla\phi(x)\right\rangle dx\\=\int\rho_\tau^{n+1}(x)\left[(\nabla\cdot \xi)(x)-\langle\nabla V(x),\xi(x)\rangle \right]dx \label{eqwithxi}
\end{equation}
\item Jetzt sei $\xi$ derart, dass für ein $\zeta\in \mathcal{C}^\infty_c(\RR^n)$ gilt $\xi=\nabla\zeta$. Es gilt (Taylor):
\begin{equation}
\zeta(\nabla\phi(x))-\zeta(x)=\left\langle\nabla\phi(x)-x,\nabla\zeta\circ\nabla\phi(x)\right\rangle+O(\vert x-\nabla\phi(x)\vert^2
\end{equation}
und damit kann die linke Seite von (\ref{eqwithxi})ersetzt werden durch
\begin{align*}
\frac{1}{\tau}\left(\int \rho_\tau^n(x)\zeta\circ\nabla\phi(x)dx-\int\rho_\tau^n(x)\zeta(x)dx\right)+O\left(\frac{1}{\tau}\int\rho_\tau^n(x)\vert x-\nabla\phi(x)\vert^2dx\right)\\
\overset{\nabla\phi\#\rho_\tau^n=\rho_\tau^{n+1}}=\frac{1}{\tau}\left(\int\rho_\tau^{n+1}\zeta-\int\rho_\tau^n\zeta\right)+O\left(\dfrac{W_2(\rho_\tau^n,\rho_\tau^{n+1})^2}{\tau}\right)
\end{align*}
\item Nun setzt man dies in (\ref{eqwithxi}) ein und es wird aufsummiert von $n_1=\lfloor t_1/\tau\rfloor$ bis zu $n_2=\lfloor t_2/\tau\rfloor+1$ für $t_1,t_2\in[0,\infty)$ beliebige Zeiten (Grenzen: siehe Beweis der Energy Gradient Estimate):
\begin{eqnarray}
\dfrac{1}{\tau}\Bigg(\int\rho_\tau(t_2)\zeta dx-\int\rho_\tau(t_1)\zeta dx+\underset{(*)}{\underbrace{O\left(\sum_{n=n_1}^{n_2}W_2(\rho_\tau^n,\rho_\tau^{n+1})^2\right)}}\Bigg)\\
=\sum_{n_1}^{n_2}\int\rho_\tau^{n+1}(x)\left[(\nabla\cdot \nabla\zeta)(x)-\langle\nabla V(x),\nabla\zeta(x)\rangle \right]dx \\=\dfrac{1}{\tau}\int_{t_1}^{t_2}\int\rho_\tau(t)(\Delta\zeta-\nabla V\cdot\nabla\zeta)dx dt+O(\tau)
\end{eqnarray}
Wobei $O(\tau)$ dem gerundeten Summationsgrenzen $n_1,n_2$ geschuldet ist: Der Fehlerterm ist 
$\leq 2\tau \int\rho_\tau(\Delta\zeta-\nabla V\nabla\zeta)\leq 2\tau\Vert\rho_\tau\Vert_1\Vert\Delta\zeta-\nabla V\nabla\zeta\Vert_\infty=O(\tau)$ mithilfe der Hölderungleichung,\\
wobei ja $\Delta\zeta$ und $\nabla V\cdot\nabla\zeta$  beschränkte Funktionen sind(kompakter Support) und $\rho_\tau(t)=\rho_\tau(t,\cdot)$ Wahrscheinlichkeitsmaß auf $\RR^n$ ist. (ähnliche Argumente, um im Vorangegangenen Grenzwerte und Integrale vertauschen zu dürfen)\\\\
\item Die Total Square Estimate bzgl. $W_2$ liefert uns 
\[(*)=O(2\tau(E(\rho_0)-\inf E)=O(\tau).\]
Damit gilt schließlich für alle festen $t_1,~t_2\in [0,\infty)$:
\begin{equation}
\int\rho_\tau(t_2)\zeta-\int\rho_\tau(t_1)\zeta+O(\tau)=\int_{t_1}^{t_2}\int\rho_\tau(t)(\Delta\zeta-\nabla V\cdot\nabla\zeta)dt+O(\tau)
\end{equation}
\item Grenzwert $\tau\to 0$ liefert:
\begin{equation}
\int\rho(t_2)\zeta-\int\rho(t_1)\zeta=\int_{t_1}^{t_2}\int\rho(t)(\Delta\zeta-\nabla V\cdot\nabla\zeta)dt
\end{equation}
Setze $t_1=0$: Der Grenzwert der Approximation erfüllt nach diesem Beweis die schwache Formulierng der linearen Fokker-Planck-Gleichung mit Potential V. Da die Lösung eindeutig ist, haben wir die (LFP) gelöst. 


\end{enumerate}
\end{proof}


\end{document}
