\documentclass[11pt,a4paper,notitlepage]{scrreprt}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{ulsy}
\usepackage{tikz}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath,amssymb,amstext,amsfonts,mathrsfs, amsthm}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{color}
\usepackage{enumitem}
\usepackage{framed}


% Mathmatische Symbole
\newcommand{\RR}{\mathbb{R}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\ee}{\operatorname{e}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\grad}{\operatorname{grad}}

\newtheorem{defi}{Definition}[section]
\newtheorem{prop}[defi]{Proposition}
\newtheorem{theorem}[defi]{Theorem}
\newtheorem{cor}[defi]{Corollar}
\newtheorem{lem}[defi]{Lemma}
\newtheorem{bem}[defi]{Bemerkung}

\author{ich}
\title{Seminar 0.0}



\begin{document}
\parindent 0pt


%\maketitle
%\newpage
%\tableofcontents
%\newpage

\pagestyle{plain}



 Prof. Dr. Matthes \hfill Manuela Lambacher\\
 Optimaler Transport - Geometrie und Anwendungen \hfill xx.12.2015
 \begin{center}
  {\huge{tolles referat}} 
 \end{center}
 
 \hrule
 
\numberwithin{equation}{section}
\renewcommand{\thechapter}{\arabic{section}}
\renewcommand{\thesection}{\arabic{section}}
\section{Generalisierte Gradientenflüsse}

Wir betrachten folgende partielle Diffgleichung: 
\begin{eqnarray}
\dfrac{dX}{dt}=~-\grad E(X) \label{eq1}
\end{eqnarray}
$E$ ist hierbei die Energie im Bezug auf den Optimalen Transport, die Diffgl beschreibt z.b. die kinetische Energie von Partikeln. 
TODO: Räume, was woraus... 
\newline

Um dieses Problem nicht explizit zu lösen, wollen wir es zeitlich diskretisieren, sodann zum Limes übergehen während unser Zeitschritt gegen Null geht. Dabei können subdifferentiale und tangentenräume schön umgangen werden und der GradientenOperator nicht explizit benutzt werden.

\begin{defi}
Der approximierte Gradientenfluss für ein Energie-Funktional E in einem abstraktem Metrischen Raum mit einer metric-denoted Distanz sei gegeben durch:
\newline
Sei der timestep $\tau > 0$, dann ist die Folge $\left( X^n_\tau \right)_{n\geq 0}$:
\\
$X_\tau^0:=X_0,$
\\
$X_\tau^{n+1}$ ist der Minimierer (oder ein Minimierer, wir verlangen keine Eindeutigkeit TODO: ex eine lösung?) von 
\begin{eqnarray}
\min\left[E(X)+\dfrac{\dist(X_\tau^n,X)^2}{2\tau}\right] \label{Min}
\end{eqnarray}
Sei $X_\tau$ auf $\RR_+$ als stückweise konstante Funktion mit Wert $X_\tau(t)=X^n_\tau$ für $t\in [n\tau,(n+1)\tau)$.
\end{defi}

\begin{bem}
Betrachtet man den euklidischen Abstand, ist die Euler-Lagrange-Gleichung zu dem Minimierungsproblem (\ref{Min})
\begin{eqnarray*}
\dfrac{X_\tau^{n+1}-X_\tau^n}{\tau}=-\grad E(X_\tau^{n+1})
\end{eqnarray*}
\end{bem}

Nach Aufstellen der Diskretisierung wollen wir $\tau \to 0$ gehen lassen und betrachten den Grenzwert, den "'generalisierten Gradientenfluss"'.
Um zu zeigen, dass dieser Grenzwert exisitert und die Ausgangsgleichung erfüllt, müssen dessen Eigenschaften untersucht werden. 

\subsection{Drei Ungleichungen}
Wir nehmen an, dass $E$ nach unten durch eine absolute Konstante ist, d.h. $E(X)\geq C$ unabh. von $X$.
\newline


$X_\tau^{n+1}$ ist ein Minimierer von dem Funktional $X \mapsto E(X)+\dfrac{\dist(X_\tau^n,X)^2}{2\tau}$, und damit ist offensichtlich das Funktional an $X_\tau^{n+1}$ kleiner gleich dem Wert dessen an der Stelle $X_\tau^n$ (Eigenschaften Metrik: $\dist(X_\tau^n,X_\tau^n)=0$). Damit haben wir
\begin{eqnarray}
E(X_\tau^{n+1})+\dfrac{\dist(X_\tau^n,X_\tau^{n+1})^2}{2\tau}\leq E(X_\tau^n). \label{ineq1}
\end{eqnarray} 
\begin{lem}[Energy estimate]
\begin{eqnarray}
\sup_{n\geq 0}E(X_\tau^n)\leq E(X^0) \label{enest}
\end{eqnarray}
\end{lem}
\begin{proof}
Folgt sofort aus (\ref{ineq1}), da der zweite Term nichtnegativ und somit die Folge der $(E(X_\tau^n)$ für n monoton abnehmend ist. 
\end{proof}

\begin{lem}[total square distance estimate]
\begin{eqnarray}
\sum_{n\geq 0}\dist(X_\tau^n,X_\tau^{n+1})^2\leq 2\tau (E(X^0)-\inf E)\label{totalsquare}
\end{eqnarray}
\end{lem}
\begin{proof}
Summation über (\ref{ineq1})
\end{proof}

\begin{bem}
Daraus kann auch eine Hölder 1/2-Abschätzung für $X_\tau$ abgeleitet werden: für $s<t$ gilt:
\begin{eqnarray}
\dist(X_\tau(s),X_\tau(t))^2 \leq \left[\dfrac{t-s}{\tau}+1\right] \sum_{\frac{s}{\tau}\leq n \leq \frac{t}{\tau}} \dist (X^n_\tau, X_\tau^{n+1})^2 \leq C[(t-s)+\tau], \label{Hölder}
\end{eqnarray}
wobei $C=2[E(X^0)-\inf E]$.
\\ 
Folgt aus Dreiecksunglichung für $W_2$ und Cauchy-Schwarz.
\end{bem}
 
\begin{lem}[Energy gradient estimate]
Sei eine zugrundeliegende Riemannstruktur gegeben und das Energie Funktional hinreichend glatt. Dann gilt 
\begin{eqnarray}
\tau \sum_{n\geq 0} \Vert\grad E(X_\tau^n)\Vert^2 \leq 2[E(X^0)-\inf E]
\end{eqnarray}
bzw. im kontinuierlichen Falle:
\begin{eqnarray}
\int_0^\infty \Vert\grad E(X_\tau^n)\Vert^2 dt\leq 2[E(X^0)-\inf E]
\label{Engrad}
\end{eqnarray}
\end{lem}
\begin{proof}
Sei $\omega$ ein beliebiger Tangentenvektor auf $X_\tau^{n+1}$. Definiere für ein kleines $\varepsilon>0$ einen "'Pfad"' $\tilde{X}_\varepsilon$ folgendermaßen:\\
\begin{eqnarray}
\tilde{X}_0=X_\tau^{n+1},~~\dfrac{d\tilde{X}_\varepsilon}{d\varepsilon}\Bigg|_{\varepsilon=0} =\omega
\end{eqnarray}
Da $X_\tau^{n+1}$ Minimierer ist, gilt offensichtlich
\begin{eqnarray}
E(X_\tau^{n+1})+\dfrac{\dist(X_\tau^n,X_\tau^{n+1})^2}{2\tau}\leq E(\tilde{X}_\varepsilon)+\dfrac{\dist(X_\tau^n,\tilde{X}_\varepsilon)^2}{2\tau}~~~\forall \varepsilon>0 \label{a)}
\end{eqnarray}
Unter der Annahme, dass das Energiefunktional glatt ist, gilt:
\begin{eqnarray}
E(\tilde{X}_\varepsilon)=E(X_\tau^{n+1})+\varepsilon\langle \grad E(X_\tau^{n+1}),\omega\rangle+O(\varepsilon^2) \label{b)}
\end{eqnarray}
Außerdem gilt:
\begin{align*}
\dist (X_\tau^n,\tilde{X}_\varepsilon)^2-\dist(X_\tau^n,X_\tau^{n+1})^2\\
=\left(\dist(X_\tau^n,\tilde{X}_\varepsilon)+\dist(X_\tau^n,X_\tau^{n+1})\right)\left(\dist(X_\tau^n,\tilde{X}_\varepsilon)-\dist(X_\tau^n,X_\tau^{n+1})\right)\\
\leq \left(\dist(X_\tau^n,\tilde{X}_\varepsilon)+\dist(X_\tau^n,X_\tau^{n+1})\right)\dist(X_\tau^{n+1},\tilde{X}_\varepsilon) \\
\leq\left(2\dist(X_\tau^n,X_\tau^{n+1}+\dist(X_\tau^{n+1},\tilde{X}_\varepsilon)\right)\dist(X_\tau^{n+1},\tilde{X}_\varepsilon)
\end{align*}
Wir wissen, dass \[\dist(X_\tau^{n+1},\tilde{X}_\varepsilon)=\varepsilon\Vert \omega \Vert + o(\varepsilon) \].
Damit wird aus obiger Gleichung
\begin{eqnarray}
\dfrac{\dist(X_\tau^n,\tilde{X}_\varepsilon)^2}{2\tau}\leq \dfrac{\dist(X_\tau^n,X_\tau^{n+1})^2}{2\tau}+\varepsilon\dist(X_\tau^n,X_\tau^{n+1})\dfrac{\Vert \omega \Vert}{\tau} + o(\varepsilon) \label{c)}
\end{eqnarray}
\\
Jetzt kombinieren wir die drei Aussagen \ref{a)}\ref{b)}\ref{c)}: Dazu setzen wir zunächst \ref{b)}in \ref{a)}ein.
\begin{align*}
E(X_\tau^{n+1})+\dfrac{\dist(X_\tau^n,X_\tau^{n+1})^2}{2\tau}\leq E(X_\tau^{n+1})+\varepsilon\langle \grad E(X_\tau^{n+1}),\omega\rangle+O(\varepsilon^2)+\dfrac{\dist(X_\tau^n,\tilde{X}_\varepsilon)^2}{2\tau}
\\
\Rightarrow \dfrac{\dist(X_\tau^n,X_\tau^{n+1})^2}{2\tau}\overset{\ref{c)}}{\leq} \varepsilon\langle \grad E(X_\tau^{n+1}),\omega\rangle+O(\varepsilon^2)+ \dfrac{\dist(X_\tau^n,X_\tau^{n+1})^2}{2\tau}+\varepsilon\dist(X_\tau^n,X_\tau^{n+1})\dfrac{\Vert \omega \Vert}{\tau} + o(\varepsilon)\\
\Rightarrow 0\leq \varepsilon \left(\langle \grad E(X_\tau^{n+1}),\omega\rangle)+\dist(X_\tau^n,X_\tau^{n+1})\dfrac{\Vert \omega \Vert}{\tau} +\underset{\to 0 \text{ für } \varepsilon \to 0} {\underbrace{\dfrac{o(\varepsilon)}{\varepsilon}+\dfrac{O(\varepsilon^2)}{\varepsilon}}} \right)
\end{align*}
Nun wählen wir das explizite $\omega=-\grad E(X_\tau^{n+1})$ (warum ist das im geforderten raum?)
\begin{align*}
0\leq \underset{-\Vert \grad E(X_\tau^{n+1}) \Vert^2}{\underbrace{\langle \grad E(X_\tau^{n+1}),-\grad E(X_\tau^{n+1}\rangle)}}+\dfrac{\dist(X_\tau^n,X_\tau^{n+1})\Vert \grad E(X_\tau^{n+1}) \Vert}{\tau}\\
\Rightarrow \dfrac{\dist(X_\tau^n,X_\tau^{n+1})^2}{\tau^2} \geq \Vert \grad E(X_\tau^{n+1}) \Vert^2
\end{align*}
In Verbindung mit der square distance estimate (\ref{totalsquare}) kommen wir schließlich zu der geforderten Gleichung:
\begin{equation}
 2\tau (E(X^0)-\inf E)\geq \sum_{n\geq 0} \dist(X_\tau^n,X_\tau^{n+1})^2 \geq \tau^2 \sum_{n\geq 1} \Vert \grad E(X_\tau^n)\Vert^2
\end{equation}
TODO: warum kann ich den term für n=0 auch mitnehmen? also summe über $n\geq 0$?
\end{proof}
\begin{bem}
Die kontinueriliche Funktion gilt sogar in einer nicht-Riemanschen Situation mit einer passenden Definition der Norm des Gradienten. (todo: riemann setting häh?) 
\end{bem}

Diese drei Abschätzungen sollten die relative Kompaktheit von $(X_\tau)$ für $\tau \to 0$ sicherstellen. \\
Um aber zum Limes überzugehen, müssen wir ein dem letzten Beweis ähnliches Prozedere unterlaufen: Kleine perturbationen, $\omega$ beliebig lassen. Eine approximierte euler-lagrange-gl finden. siehe nächstes kapteil. außerdem: ???? 
\section{Anwendung auf das Monge-Kantorovich Problem}
Die oben erwähnte Strategie wurde zum ersten mal am Beispiel der linearen Fokker-Plank-Gleichung angewandt. Dies liefert interessante EInblicke in die Methode. Die übrigens auch implementiert werden kann. \\

setting: \\
Betrachte das Energie-Funktional \[E(\rho)=\int \rho \log \rho + \int\rho V\] auf $P(\RR^k)$, wobei $V(x)$ ein glattes Potential (??) ist, das folgende Wachstumsbedingung erfüllt: $V(x)=O(\vert x \vert^2)$ für $x\to\infty$.\\
Sei $\rho_0$ eine Wahrscheinlichkeitsdichte als Anfangswert mit endlichem $E(\rho)$.\\
Sei weiterhin $\tau>0$ und definiere sodann die Folge $(\rho_\tau^n)$ analog zu Punkt 1 des Vortrages:\\

\[\rho_\tau^0=\rho^0; \]
gegeben $\rho_\tau^n$ definiere den nächsten Schritt als Minimierer von
\begin{equation*} \rho \mapsto E(\rho)+\dfrac{W_2(\rho_\tau^n,\rho)^2}{2\tau}
\end{equation*}
Da E strikt konvex im üblichen Sinne ist (displacement konvex nur wenn V konvex ist), ebenso wie die Wassersteinmetrik, und damit ist unser Minimierer eindeutig. \\
Wie im vorherigen Abschnitt kann man eine uniform control über $\sup_{t\geq 0} E(\rho_\tau(t))$ erlangen, die einem die tightness der familie $(rho_\tau)_{\tau >0}$ in der x-varibale sowie schwache $L^1$-kompaktheit garantiert. \\
Die approximate Hölder 1/2 continuity estimate in der quadraticshen wasserstein distanz sichert die approximative uniforme equikontinuität in zeit von $(\rho_\tau)_\tau$. All dies ist hinreichend um (betrachte evtl subsequence $(\tau_k)_k$) $(\rho_\tau)_\tau$ zu einer Funktion $\rho: \RR_+ \mapsto P_{ac}(\RR^n)$ konvergiert, stetig falls $P_{ac}(\RR^n)$ mit der schwachen $L^1$-Topologie verknüft ist. \\
Um nun den Grenzewrt zu betrachten, brauche nwir eine geeignete Variation von $\rho_\tau^{n+1}$.\\
Sei $\xi$ ein glattes Vektorfeld mit kompaktem Träger, und $T_\varepsilon$ die Familie von Trajektoren verknüpft mit dem Vektorfeld $\operatorname{Id} +\varepsilon\xi$. Dann definiere
\[\tilde{\rho}_\varepsilon=T_\varepsilon \#\rho_\tau^{n+1} \]
Für $\varepsilon$ klein genug ist $T_\varepsilon$ ein $C^1$-diffeomorphismus (fixed point theoren und $\det(\nabla T_\varepsilon)>0$.\\
Nach Definition gilt:
\begin{eqnarray}
E(\tilde{\rho}_\varepsilon)=&\int\tilde{\rho}_\varepsilon\log\tilde{\rho}_\varepsilon+\int\tilde{\rho}_\epsilon V\\
=&\int \rho_\tau^{n+1}\log\dfrac{\rho_\tau^{n+1}}{\det(I_k+\varepsilon \nabla \xi)}+\int \rho_\tau^{n+1}(x)V(x+\varepsilon\xi(x))dx.
\end{eqnarray}
Aus der energy estimate folgt andererseits, dass $\rho_\tau^n, \rho_\tau^{n+1}$ absolut stetig sind. Damit existert eine optimale Abbildung $\nabla \phi$ s.t. $\nabla \phi \# \rho_\tau^n=\rho_\tau^{n+1}$ und es gilt
\begin{equation*}
W_2(\rho_\tau^n,\tilde{\rho}_\varepsilon)^2 \leq \int \rho_\tau^n(x)\vert x-\nabla \phi(x)-\varepsilon\xi\circ\nabla\phi(x)\vert^2dx
\end{equation*}
Damit kommen wir auf
\begin{align}
\dfrac{W_2(\rho_\tau^n,\tilde{\rho}_\varepsilon)^2}{2\tau}&+E(\tilde{\rho}_\varepsilon)-\dfrac{W_2(\rho_\tau^n,\rho_\tau^{n+1})^2}{2\tau}-E(\rho_\tau^{n+1})\\
\leq\int&\rho_\tau^n(x)\left(\dfrac{\vert x-\nabla \phi(x)-\varepsilon\xi\circ\nabla\phi(x)\vert^2-\vert x-\nabla\phi(x)\vert^2}{2\tau}\right)dx\\
+&\int\rho_\tau^{n+1}(x)[V(x+\varepsilon\xi(x))-V(x)]dx-\int\rho_\tau^{n+1}(x)\log \det(\operatorname{Id}+\varepsilon\nabla\xi(x))dx\geq 0
\end{align}
wobei die nichtnegativität der linken Seite aus der Minimierungseigenschaft von $\rho_\tau^{n+1}$ folgt.
Dividiere durch $\varepsilon$ und lasse dieses gegen $0^+$ gehen, so bekommen wir: 
\begin{align*}
0\leq\frac{1}{\tau} \int \rho_\tau^n(x)\left\langle\nabla\phi(x)-x,\xi\circ\nabla\phi(x)\right\rangle dx+\int\rho_\tau^{n+1}(x)\langle\nabla V(x),\xi(x)\rangle dx\\-\int\rho_\tau^{n+1}(x)(\nabla\cdot \xi)(x)dx
\end{align*}
Da dies genauso für $-\xi$ gilt, bekommen wir Gleichheit. Umforuliert:
\begin{equation}
\frac{1}{\tau} \int \rho_\tau^n(x)\left\langle\nabla\phi(x)-x,\xi\circ\nabla\phi(x)\right\rangle dx\\=\int\rho_\tau^{n+1}(x)\left[(\nabla\cdot \xi)(x)-\langle\nabla V(x),\xi(x)\rangle \right]dx \label{eqwithxi}
\end{equation}

Jetzt sei $\xi$ derart, dass für ein $\zeta\in\mathcal{D}(\RR^n)$ gilt $\xi=\nabla\zeta$. Es gilt (erweiterung):
\begin{equation}
\zeta(\nabla\phi(x))-\zeta(x)=\left\langle\nabla\phi(x)-x,\nabla\zeta\circ\nabla\phi(x)\right\rangle+O(\vert x-\nabla\phi(x)\vert^2
\end{equation}
und damit kann die linke Seite von (\ref{eqwithxi})ersetzt werden durch
\begin{align*}
\frac{1}{\tau}\left(\int \rho_\tau^n(x)\zeta\circ\nabla\phi(x)dx-\int\rho_\tau^n(x)\zeta(x)dx\right)+O\left(\frac{1}{\tau}\int\rho_\tau^n(x)\vert x-\nabla\phi(x)\vert^2dx\right)\\
=\frac{1}{\tau}\left(\int\rho_\tau^{n+1}\zeta-\int\rho_\tau^n\zeta\right)+O\left(\dfrac{W_2(\rho_\tau^n,\rho_\tau^{n+1})^2}{\tau}\right)
\end{align*}
Setzt dies in(\ref{eqwithxi}) ein und  nun wird aufsummiert für $t_1,t_2$ beliebige Zeiten von $n_1=[t_1/\tau]$ bis zu $n_2=[t_2/\tau]$:
\begin{eqnarray}
\int\rho_\tau(t_2)\zeta-\int\rho_\tau(t_1)\zeta+O\left(\sum_{n=n_1}^{n_2}W_2(\rho_\tau^n,\rho_\tau^{n+1})^2\right)\\
=\int_{t_1}^{t_2}\int\rho_\tau(t)(\nabla\zeta-\nabla V\cdot\nabla\zeta)dt+O8\tau)
\end{eqnarray}
Wobei wir benutzt haben, dass $\nabla\zeta$ und $\nabla V\cdot\nabla\zeta$ beschränkte Funktionien sind und $\rho_\tau(t)=\rho_\tau(t,\cdot)$ Wahrscheinlichkeitsmaß auf $\RR^n$





\end{document}
